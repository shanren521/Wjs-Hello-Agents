# 大语言模型基础

## 3.1 语言模型与Transformer架构

### 3.1.1 从N-gram到RNN

语言模型是自然语言处理的核心，其根本任务是计算一个词序列出现的概率。

#### (1) 统计语言模型与N-gram的思想

正在深度学习兴起之前，统计方法是语言模型的主流。核心思想是，一个句子出现的概率，等于该句子中每个词出现的条件概率的连乘。对于一个由词w1，w2,...,w<sub>m</sub>
构成的句子S，其概率P(S)可以表示为： 

P(S)=P(w1,w2,...,wm)=P(w1)*P(w2|w1)*P(w3|w1,w2)•••P(wm|w1,...,$w_{m-1}$)   概率的链式法则

![马尔可夫假设示意图.png](../../images/马尔可夫假设示意图.png)

核心思想是：我们不必回溯一个词的全部历史，一个词的出现概率只与它前面有限的n-1个词有关基于这个假设建立的语言模型，我们称之为N-gram模型

+ Bigram(当N=2时)：
P($w_{i}$|$w_{1}$,...,$w_{i-1}$)≈P($w_{i}$|$w_{i-1}$)      一个词的出现与它前面的一个词有关

+ Trigram(当N=3时)：
P($w_{i}$|$w_{1}$,...,$w_{i-1}$)≈P($w_{i}$|$w_{i-2}$,$w_{i-1}$)   一个词的出现与它前面的两个词有关

这些概率可以通过在大型语料库中进行最大似然MLE估计来计算
P($w_{i}$|$w_{i-1}$)=Count($w_{i-1}$, $w_{i}$) / Count($w_{i-1}$)

这里的Count()函数就代表计数：
+ Count($w_{i-1}$, $w_{i}$): 表示词对($w_{i-1}$, $w_{i}$)在语料库中连续出现的总次数
+ Count($w_{i-1}$): 表示单个词$w_{i-1}$在语料库中出现的总次数

```python
import collections

# 示例语料库，与上方案例讲解中的语料库保持一致
corpus = "datawhale agent learns datawhale agent works"
tokens = corpus.split()
total_tokens = len(tokens)

# --- 第一步：计算 P(datawhale) ---
count_datawhale = tokens.count("datawhale")
p_datawhale = count_datawhale / total_tokens
print(f"第一步: P(datawhale) = {count_datawhale}/{total_tokens} = {p_datawhale:.3f}")

# --- 第二步:计算 P(agent|datawhale) ---
# 先计算 bigrams 用于后续步骤
bigrams = zip(tokens, tokens[1:])
bigrams_counts = collections.Counter(bigrams)
count_datawhale_agent = bigrams_counts[("datawhale", "agent")]
# count_datawhale 已在第一步计算
p_agent_given_datawhale = count_datawhale_agent / count_datawhale
print(f"第二步: P(agent|datawhale) = {count_datawhale_agent}/{count_datawhale} = {p_agent_given_datawhale:.3f}")

# --- 第三步:计算 P(learns|agent) ---
count_agent_learns = bigrams_counts[("agent", "learns")]
count_agent = tokens.count("agent")
p_learns_given_agent = count_agent_learns / count_agent
print(f"第三步: P(learns|agent) = {count_agent_learns}/{count_agent} = {p_learns_given_agent:.3f}")

# --- 最后:将概率连乘 ---
p_sentence = p_datawhale * p_agent_given_datawhale * p_learns_given_agent
print(f"最后: P('datawhale agent learns') ≈ {p_datawhale:.3f} * {p_agent_given_datawhale:.3f} * {p_learns_given_agent:.3f} = {p_sentence:.3f}")
```

N-gram简单有效，但是有两个致命的缺陷：
+ 数据稀疏性：如果一个词从未在语料库中出现，其概率估计就为0，这是不合理的。
+ 泛化能力差：模型无法理解词与词之间的语义相似性。例如，即使模型在语料库中见过很多次 agent learns，它也无法将这个知识泛化到语义相似的词上。当我们计算 robot learns 的概率时，如果 robot 这个词从未出现过，或者 robot learns 这个组合从未出现过，模型计算出的概率也会是零。模型无法理解 agent 和 robot 在语义上的相似性。

#### (2)神经网络语言模型与词嵌入

+ 前馈神经网络语言模型，核心思想分为两步：
  + 构建一个语义空间：创建一个高维的连续向量空间，然后将词汇表中的每个词都映射为该空间中的一个点。这个点-向量被称为词嵌入或词向量，在这个空间里，语义上相近的词，它们对应的向量在空间中的位置也相近
  + 学习从上下文到下一个词的映射：利用神经网络的强大拟合能力，来学习一个函数。这个函数的输入是前n-1个词的词向量，输出是词汇表中每个词在当前上下文后出现的概率分布。
  + ![神经网络语言模型架构示意图.png](../../images/神经网络语言模型架构示意图.png)
+ 在这个架构中，词嵌入是在模型训练过程中自动学习得到的。模型为了完成预测下一个词这个任务，会不断调整每个词的向量位置，最终使这些向量能蕴含丰富的语义信息。一旦我们将词转换成了向量，我们就可以用数学工具来度量它们之间的关系。最常用的方法是余弦相似度，通过计算两个向量的夹角的余弦值来衡量它们的相似性
  + similarity(a,b)=cos(θ)=(a\*b) / (|a|\*|b|)
  + 公式含义：
    + 如果两个向量方向完全相同，夹角为0，余弦值为1，表示完全相关
    + 如果两个向量方向正交，夹角为90，余弦值为0，表示毫无关系
    + 如果两个向量方向完全相反，夹角为180，余弦值为-1，表示完全负相关
  + 代码：
    + ```python
      import numpy as np
      
      def cosine_similarity(vec1, vec2):
        dot_product = np.dot(vec1, vec2)
        norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)
        return dot_product / norm_product
      
      # 假设已经有简化的二维词向量
      embeddings = {
        "king": np.array([0.9, 0.8]),
        "queen": np.array([0.9, 0.2]),
        "man": np.array([0.7, 0.9]),
        "woman": np.array([0.7, 0.3])
      }
      # king - man + woman
      result_vec = embeddings["king"] - embeddings["man"] + embeddings["woman"]
      
      # 计算结果向量与"queen"的相似度
      sim = cosine_similarity(result_vec, embeddings["queen"])
      print(f"king - man + woman 的结果向量: {result_vec}")
      print(f"该结果与 'queen' 的相似度: {sim:.4f}")
    ```
+ 成功解决了N-gram模型的泛化能力差的问题，但是上下文窗口是固定的，智能考虑固定数量的前文

#### (3) 循环神经网络(RNN)与长短时记忆网络(LSTM)

核心思想：为网络增加记忆能力
RNN的设计引入了一个隐藏状态向量，可以理解为短期记忆，在处理序列的每一步，网络都会读取当前的输入词，并结合它上一刻的记忆(上个时间步的隐藏状态)，然后生成一个新的记忆(当前时间步的隐藏状态)传递给下一刻，使得信息可以在序列中不断向后传递
![RNN 结构示意图.png](../../images/RNN%20结构示意图.png)

+ 标准的RNN在实践中存在一个严重的问题：长期依赖问题。在训练过程中，模型需要通过反向传播算法根据输出端的误差来调整网络深处的权重。对于RNN，序列的长度就是网络的深度，
  当序列很长时，梯度在从后向前传播的过程中会经过中会经过多次连乘，这回导致梯度值快速趋向于零(梯度消失)或变的极大(梯度爆炸)。
  梯度消失使得模型无法有效学习到序列早期信息对后期输出的影响,即难以捕捉长距离的依赖关系。

+ 为了解决长依赖问题，长短时记忆网络LSTM被设计出来。LSTM是特殊的RNN，核心是引入了细胞状态和一套精密的门控机制。细胞状态可以看作是一条独立于隐藏状态的信息通路，允许信息在时间步之间更顺畅地传递。
  门控机制则是由几个神经网络构成，它们可以学习如何有选择地让信息通过，从而控制细胞状态中信息的增加与移除
  + 遗忘门：决定从上一时刻的细胞状态中丢弃哪些信息
  + 输入门：决定将当前输入中的哪些新信息存入细胞状态
  + 输出门：决定根据当前的细胞状态，输出哪些信息到隐藏状态

### 3.1.2 Transformer架构解析

RNN与LSTM的瓶颈是必须按顺序处理数据，第t个时间步的计算，必须等待第t-1个时间步完成后才能开始。RNN无法进行大规模的并行计算，在处理长序列时效率低下，
即大地限制了模型规模和训练速度的提升。Transformer抛弃了循环结构，引入了***注意力Attention***的机制来捕捉序列内的依赖关系

#### (1) Encoder-Decoder整体结构

最初的Transformer模型是为了端到端任务机器翻译而设计的，宏观上遵循了经典的***编码器-解码器***架构

![Transformer 整体架构图.png](../../images/Transformer%20整体架构图.png)

+ 编码器：任务是***理解***输入的整个句子。它会读取所有输入词元，最终为每个词元生成一个富含上下文信息的向量表示
+ 解码器：任务是***生成***目标句子。它会参考自己已生成的前文，并咨询编码器的理解结果，来生成下一个词。

```python
import torch
import torch.nn as nn
import math

# --- 占位符模块 ---

class PositionalEncoding(nn.Module):
    """位置编码模块"""
    def forward(self, x):
        pass

class MultiHeadAttention(nn.Module):
    """多头注意力机制模块"""
    def forward(self, query, key, value, mask):
        pass
    
class PositionWiseFeedForward(nn.Module):
    """位置前馈网络模块"""
    def forward(self, x):
        pass

# --- 编码器核心层 ---

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention()  # 待实现
        self.feed_forward = PositionWiseFeedForward()  # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # 残差连接与层归一化
        # 1、多头自注意力
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2、前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        return x

# --- 解码器核心层 ---
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention()
        self.cross_attn = MultiHeadAttention()
        self.feed_forward = PositionWiseFeedForward()
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)


    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # 1、掩码多头自注意力(对自己)
        # 先残差，后归一化 可能会训练不稳定、需要学习率预热、梯度可能爆炸
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # 先归一化，后残差  
        attn_output = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)
        x = x + self.dropout(attn_output)

        # 2、交叉注意力(对编码器输出)
        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))
        
        # 3、前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

#### (2) 从自注意力到多头注意力

自注意力机制：允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的注意力权重。权重越高的词，代表其与当前词的关联性越强，
            其信息也应该在当前词的表示中占据更大的比重

自注意力机制为每个输入的词元向量引入了三个可学习的角色：
+ 查询(Query, Q): 代表当前词元，它正在主动地查询其他词元以获取信息。
+ 键(Key, K): 代表句子中可被查询的词元标签或索引
+ 值(Value, V): 代表词元本身所携带的内容或信息

这三个向量都是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵($W^Q$, $W^K$, $W^V$)得到的。
+ 对于句子中的每个词，都通过权重矩阵生成其Q，K，V向量
+ 计算相关性得分：要计算词A的新表示，就用词A的Q向量，去和句子中所有词(包括A自己)的K向量进行点积运算。这个得分反映了其他词对于理解词A的重要性
+ 稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_k}$ (${d_k}$是K向量的维度)，以防止梯度过小，然后用softmax函数将分数转换成总和为1的权重，
                也就是归一化的过程。
+ 加权求和：将上一步得到的权重分别乘以每个词对应的V向量，然后将所有结果相加。最终得到的向量，就是词A融合了全局上下文信息后的新表示。
    
$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\text{V}
$$

如果只进行一次上述的注意力计算(即单头)，模型可能会只学会关注一种类型的关联。所以产生了多头注意力机制。思想：把一次做完变成分成几组，分开做，再合并。
它将原始的Q，K，V向量在维度上切分成H份(H就是头数)，每一份都独立进行一次单头注意力计算。每个头都能捕捉到一种不同的特征关系。最后将这h个输出向量拼接起来，
再通过一个线性变换进行整合，就得到了最终的输出。

![多头注意力机制.png](../../images/多头注意力机制.png)

+ ```python
  class MultiHeadAttention(nn.Module):
    """多头注意力机制模块"""
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model 必须能被num_heads整除"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
  
        # 定义Q，K，V和输出的线性变换层
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
  
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # 1、计算注意力得分(QK^T)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # 2、应用掩码
        if mask is not None:
            # 将掩码中为0的位置设置为一个非常小的负数，这样softmax后会接近0
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)
  
        # 3、计算注意力权重(Softmax)
        attn_probs = torch.softmax(attn_scores, dim=-1)
        
        # 4、加权求和(权重*V)
        output = torch.matmul(attn_probs, V)
        return output
  
    def split_heads(self, x):
        # 将输入x的形状从(batch_size, seq_length, d_model)
        # 变为(batch_size, num_heads, seq_length, d_k)
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
  
    def combine_heads(self, x):
        # 将输入x的形状(batch_size, num_heads, seq_length, d_k)
        # 变为(batch_size, seq_length, d_model)
        batch_size, num_heads, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
  
    def forward(self, Q, K, V, mask=None):
        # 1、对Q、K、V进行线性变换
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))
        
        # 2. 计算缩放点积注意力
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # 3. 合并多头输出并进行最终的线性变换
        output = self.W_o(self.combine_heads(attn_output))
        return output
  ```

#### (3)前馈神经网络

在每个Encoder和Decoder层中，多头注意力子层之后都跟着一个逐位置前馈网络FFN，注意力层的作用是从整个序列中动态的聚合相关信息，前馈网络的作用是从
这些聚合后的信息中提取更高阶的特征。

前馈网络会独立地作用于序列中的每一个词元向量。对于一个长度为seq_len的序列，FFN会被调用seq_len次，每次处理一个词元。重要的是，所有位置共享的是
同一组网络权重。这种设计既保持了对每个位置进行独立加工的能力，又大大减少了模型的参数量。由两个线性变换和一个ReLU激活函数组成：
$$
\text{FFN(x)}=max(0,x{W_1}+{b_1}){W_2}+{b_2}
$$

其中，x是注意力子层的输出。${W_1}$,${b_1}$,${W_2}$,${b_2}$是可学习的参数。通常，第一个线性层的输出为度d_ff会远大于输入的维度d_model，经过
ReLU激活后再通过第二个线性层映射回d_model维度。先扩大再缩小的模式，有助于模型学习更丰富的特征表示。

```python
class PositionWiseFeedForward(nn.Module):
    """位置前馈网络模块"""
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionWiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # x形状: (batch_size, seq_len, d_model)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        # 最终输出形状：(batch_size, seq_len, d_model)
        return x
```

#### (4)残差连接与层归一化

在Transformer的每个编码器和解码器中，所有子模块都被一个Add & Norm操作包裹，这个组合是为了保证Transformer能够稳定训练。
这个操作由两个部分组成：
+ 残差连接(Add): 将子模块的输入x直接加到该子模块的输出Sublayer(x)上。解决了深度神经网络中的梯度消失问题。在反向传播时，梯度可以绕过子模块直接向前传播，
                从而保证了即使网络层数很深，模型也能得到有效的训练。公式为：Output = x + Sublayer(x)
+ 层归一化(Norm): 对单个样本的所有特征进行归一化，使其均值为0，方差为1。解决了模型训练过程中的内部协变量偏移问题，加速模型收敛并提高训练的稳定性。

#### (5) 位置编码

自注意力机制只关心词元之间的关系，忽略了它们的排列。为了解决这个问题引入了位置编码

核心思想：









